version: '3.4'

# networks:
#   default:
#     name: multimodal
#     external: true

volumes:
  elasticsearch-data:
    driver_opts:
      type: none
      o: bind
      device: ./elasticsearch_volume

secrets:
  elasticsearch.keystore:
    file: ../elastdocker/secrets/keystore/elasticsearch.keystore
  elasticsearch.service_tokens:
    file: ../elastdocker/secrets/service_tokens
  elastic.ca:
    file: ../elastdocker/secrets/certs/ca/ca.crt
  elasticsearch.certificate:
    file: ../elastdocker/secrets/certs/elasticsearch/elasticsearch.crt
  elasticsearch.key:
    file: ../elastdocker/secrets/certs/elasticsearch/elasticsearch.key
  kibana.certificate:
    file: ../elastdocker/secrets/certs/kibana/kibana.crt
  kibana.key:
    file: ../elastdocker/secrets/certs/kibana/kibana.key

services:

  document_population:
    build: ../document_population
    stdin_open: true
    tty: true
    volumes:
      - ../document_population/src:/src
      - ../document_population/data:/data
    # ports:
    #   - 8888:8880
    ulimits:
      memlock: -1 # set upper limit for how much memory is locked for the container (-1 means lock as much as the container uses)
    shm_size: 32gb # set upper limit for how much shared memory container can use

  mtcnn:
    # image: mtcnn
    build: ../visual_entity_extraction/facenet_triton/mtcnn

    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    ports:
      - 5000:5000
    command: python server.py
    # networks:
    #     - backend    

  triton:
    # image: triton
    build: ../triton
    stdin_open: true
    tty: true
    volumes:
      - ../triton/models:/models
    ulimits:
      memlock: -1 # set upper limit for how much memory is locked for the container (-1 means lock as much as the container uses)
    shm_size: 16gb # set upper limit for how much shared memory container can use
    ports:
      - 8000:8000
      - 8001:8001
      - 8002:8002
    command: tritonserver --model-repository=/models
    # restart: always
    # networks:
    #     - backend    

  face_id:
    build: ../visual_entity_extraction/facenet_triton/face_id_api
    ports:
      #to the host
      - 8004:8000 #host:container
    stdin_open: true
    tty: true
    ulimits:
      memlock: -1 # set upper limit for how much memory is locked for the container (-1 means lock as much as the container uses)
    shm_size: 4gb

    volumes:
      - ../visual_entity_extraction/facenet_triton/data:/data
    command: uvicorn main:api --host 0.0.0.0
    # restart: always

  obj_det:
    build: ../visual_entity_extraction/yolo_api/obj_det_api
    ports:
      #to the host
      - 8005:8000 #host:container
    stdin_open: true
    tty: true
    ulimits:
      memlock: -1 # set upper limit for how much memory is locked for the container (-1 means lock as much as the container uses)
    shm_size: 4gb

    volumes:
      - ../visual_entity_extraction/yolo_api/data:/data
    command: uvicorn main:api --host 0.0.0.0
    # restart: always
    #entity linking service

  blink:
    build: ../text_entity_extraction/
    stdin_open: true
    tty: true
    ports:
      - 5050:5050
    command: uvicorn api_service:app --host 0.0.0.0 --port 5050
    volumes:
      - ../text_entity_extraction/BLINK_api/models:/BLINK_api/models
      - ../text_entity_extraction/BLINK_api/logs:/BLINK_api/logs
      - ../text_entity_extraction/BLINK_api/configs:/BLINK_api/configs
      - ../text_entity_extraction/BLINK_api/src:/BLINK_api/src
    environment:
      NVIDIA_VISIBLE_DEVICES: 0
      CUDA_VISIBLE_DEVICES: 0
    ulimits:
      memlock: -1
    shm_size: '16gb'

  #entity recogniton and relation extration
  jerex-api:
    build: ../text_entity_extraction/multimodal-jerex/
    stdin_open: true
    tty: true
    networks:
      - default
    ports:
      - 8080:8080
    command: uvicorn api_service:app --host 0.0.0.0 --port 8080
    environment:
      NVIDIA_VISIBLE_DEVICES: 0
      CUDA_VISIBLE_DEVICES: 0
    ulimits:
      memlock: -1
    shm_size: '16gb'
    volumes:
      - ../text_entity_extraction/multimodal-jerex:/jerex
  #raw document store and entity store
  elasticsearch:
    image: elastdocker/elasticsearch:${ELK_VERSION}
    build:
      context: ../elastdocker/elasticsearch/
      args:
        ELK_VERSION: ${ELK_VERSION}
    restart: unless-stopped
    environment:
      ELASTIC_USERNAME: ${ELASTIC_USERNAME}
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}
      ELASTIC_CLUSTER_NAME: ${ELASTIC_CLUSTER_NAME}
      ELASTIC_NODE_NAME: ${ELASTIC_NODE_NAME}
      ELASTIC_INIT_MASTER_NODE: ${ELASTIC_INIT_MASTER_NODE}
      ELASTIC_DISCOVERY_SEEDS: ${ELASTIC_DISCOVERY_SEEDS}
      ES_JAVA_OPTS: "-Xmx${ELASTICSEARCH_HEAP} -Xms${ELASTICSEARCH_HEAP} -Des.enforce.bootstrap.checks=true -Dlog4j2.formatMsgNoLookups=true"
      bootstrap.memory_lock: "true"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
      - ../elastdocker/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      - ../elastdocker/elasticsearch/config/log4j2.properties:/usr/share/elasticsearch/config/log4j2.properties
    secrets:
      - source: elasticsearch.keystore
        target: /usr/share/elasticsearch/config/elasticsearch.keystore
      - source: elasticsearch.service_tokens
        target: /usr/share/elasticsearch/config/service_tokens
      - source: elastic.ca
        target: /usr/share/elasticsearch/config/certs/ca.crt
      - source: elasticsearch.certificate
        target: /usr/share/elasticsearch/config/certs/elasticsearch.crt
      - source: elasticsearch.key
        target: /usr/share/elasticsearch/config/certs/elasticsearch.key
    ports:
      - "9200:9200"
      - "9300:9300"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 200000
        hard: 200000
    healthcheck:
      test:
        [
          "CMD",
          "sh",
          "-c",
          "curl -sf --insecure https://$ELASTIC_USERNAME:$ELASTIC_PASSWORD@localhost:9200/_cat/health | grep -ioE 'green|yellow' || echo 'not green/yellow cluster status'"
        ]

  logstash:
    image: elastdocker/logstash:${ELK_VERSION}
    build:
      context: ../elastdocker/logstash/
      args:
        ELK_VERSION: $ELK_VERSION
    restart: unless-stopped
    volumes:
      - ../elastdocker/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - ../elastdocker/logstash/config/pipelines.yml:/usr/share/logstash/config/pipelines.yml:ro
      - ../elastdocker/logstash/pipeline:/usr/share/logstash/pipeline:ro
    secrets:
      - source: elastic.ca
        target: /certs/ca.crt
    environment:
      ELASTIC_USERNAME: ${ELASTIC_USERNAME}
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}
      ELASTICSEARCH_HOST_PORT: https://${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}
      LS_JAVA_OPTS: "-Xmx${LOGSTASH_HEAP} -Xms${LOGSTASH_HEAP} -Dlog4j2.formatMsgNoLookups=true"
    ports:
      - "5044:5044"
      - "9600:9600"
    healthcheck:
      test:
        [
          "CMD",
          "curl",
          "-s",
          "-XGET",
          "http://127.0.0.1:9600"
        ]

  kibana:
    image: elastdocker/kibana:${ELK_VERSION}
    build:
      context: ../elastdocker/kibana/
      args:
        ELK_VERSION: $ELK_VERSION
    restart: unless-stopped
    volumes:
      - ../elastdocker/kibana/config/:/usr/share/kibana/config:ro
    environment:
      ELASTIC_USERNAME: ${ELASTIC_USERNAME}
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}
      ELASTICSEARCH_HOST_PORT: https://${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}
    env_file:
      - ../elastdocker/secrets/.env.kibana.token
    secrets:
      - source: elastic.ca
        target: /certs/ca.crt
      - source: kibana.certificate
        target: /certs/kibana.crt
      - source: kibana.key
        target: /certs/kibana.key
    ports:
      - "5601:5601"

  image_server:
    build: ../image_server
    stdin_open: true
    tty: true
    volumes:
      - ../image_server/src:/src
      - ../image_server/images:/images
    # ports:
    #   - 8888:8880
    command: uvicorn main:api --host 0.0.0.0
    restart: always
    ulimits:
      memlock: -1 # set upper limit for how much memory is locked for the container (-1 means lock as much as the container uses)
    shm_size: 32gb # set upper limit for how much shared memory container can use
  # neo4j:
  #   build: neo4j

  #   environment:
  #   - NEO4JLABS_PLUGINS=${NEO4JLABS_PLUGINS}
  #   - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD}
  #   ports:
  #   - 7687:7687
  #   - 7474:7474
  #   ulimits:
  #   memlock: -1 # set upper limit for how much memory is locked for the container (-1 means lock as much as the container uses)
  #   shm_size: 32gb # set upper limit for how much shared memory container can use
